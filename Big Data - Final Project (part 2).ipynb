{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0018dd75-6c33-4f2c-b190-263ec415f321",
   "metadata": {},
   "source": [
    "### <u>**COMP-548DL**</u>\n",
    "### <u>**Big Data Management and Processing**</u>\n",
    "### <u>**Semester Project**</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b059cf52-2b80-44d9-8624-79a1829d63cc",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2670f80-b74e-4e8c-891b-aa9bf3d8566c",
   "metadata": {},
   "source": [
    "#### <u>**Part 2:**</u>\n",
    "#### For the analysis of our data we will use Spark RDDs and Spark Dataframes, which are built on top of RDDs actually. We also going to use Python's time library to compare the time it takes for RDDs and Dataframes to give us back the exact same information we asking from our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "626a84bd-728a-4878-a148-0424b93c62e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext, SparkConf\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "332f681b-a3bd-4981-89ee-5395c3896ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName('Read-files').setMaster('local[2]') \n",
    "sc = SparkContext.getOrCreate(conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "081609dd-ba14-432d-a3c2-28361e2efd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing the structure for our data\n",
    "crime_schema = StructType([\n",
    "    StructField('lsoa_code', StringType(), False),\n",
    "    StructField('borough', StringType(), False),\n",
    "    StructField('major_category', StringType(), False),\n",
    "    StructField('minor_category', StringType(), False),\n",
    "    StructField('value', IntegerType(), False),\n",
    "    StructField('year', IntegerType(), False),\n",
    "    StructField('month', IntegerType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3edfc30-dc40-48b5-8269-ebb535bd695f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = 'london_crime_json.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "740b005e-5d57-48c5-b219-3fb516c3d710",
   "metadata": {},
   "outputs": [],
   "source": [
    "crime_df=spark.read.json(path,schema=crime_schema) #reading the json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6683f70-5ae5-4ce4-94f1-6bffdf70402e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+--------------------+-----+----+-----+\n",
      "|lsoa_code|             borough|      major_category|      minor_category|value|year|month|\n",
      "+---------+--------------------+--------------------+--------------------+-----+----+-----+\n",
      "|E01001116|             Croydon|            Burglary|Burglary in Other...|    0|2016|   11|\n",
      "|E01001646|           Greenwich|Violence Against ...|      Other violence|    0|2016|   11|\n",
      "|E01000677|             Bromley|Violence Against ...|      Other violence|    0|2015|    5|\n",
      "|E01003774|           Redbridge|            Burglary|Burglary in Other...|    0|2016|    3|\n",
      "|E01004563|          Wandsworth|             Robbery|   Personal Property|    0|2008|    6|\n",
      "|E01001320|              Ealing|  Theft and Handling|         Other Theft|    0|2012|    5|\n",
      "|E01001342|              Ealing|Violence Against ...|    Offensive Weapon|    0|2010|    7|\n",
      "|E01002633|            Hounslow|             Robbery|   Personal Property|    0|2013|    4|\n",
      "|E01003496|              Newham|     Criminal Damage|Criminal Damage T...|    0|2013|    9|\n",
      "|E01004177|              Sutton|  Theft and Handling|Theft/Taking of P...|    1|2016|    8|\n",
      "|E01001985|            Haringey|  Theft and Handling|Motor Vehicle Int...|    0|2013|   12|\n",
      "|E01003076|             Lambeth|Violence Against ...|      Other violence|    0|2015|    4|\n",
      "|E01003852|Richmond upon Thames|             Robbery|   Personal Property|    0|2014|    1|\n",
      "|E01004547|          Wandsworth|Violence Against ...|    Offensive Weapon|    0|2011|   10|\n",
      "|E01002398|          Hillingdon|  Theft and Handling|Theft/Taking Of M...|    0|2016|    2|\n",
      "|E01002358|            Havering|Violence Against ...|        Wounding/GBH|    0|2012|    2|\n",
      "|E01000086|Barking and Dagenham|  Theft and Handling|  Other Theft Person|    1|2009|    5|\n",
      "|E01003708|           Redbridge|Violence Against ...|      Common Assault|    0|2009|    6|\n",
      "|E01002945|Kingston upon Thames|  Theft and Handling|    Theft From Shops|    0|2016|   11|\n",
      "|E01004195|              Sutton|               Drugs| Possession Of Drugs|    0|2009|   10|\n",
      "+---------+--------------------+--------------------+--------------------+-----+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "crime_df.show() #showing some of the rows in our data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4966d89-1c95-44db-9266-632817300853",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e664fc62-67a9-4b2f-a7d6-276cd9f0fd13",
   "metadata": {},
   "source": [
    "<h3>Spark RDD</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7816d3c5-c61e-4ff9-8a9a-768a83e49fde",
   "metadata": {},
   "source": [
    "<h4>Along with RDD, we are going to use some basic map and reduce funtions to get the data we want. We want the top10 boroughs in crimes, top5 crimes in all boroughs through out the years and the top3 years with the most crimes.All 3 MapReduce operations are identical, extracting similar results from different columns.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "961aa067-3949-4f65-9b64-cdb54ad63303",
   "metadata": {},
   "outputs": [],
   "source": [
    "crime_rdd=crime_df.rdd #creating the rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d301c55c-1ec3-49e2-bacf-f5a4863e296e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crime_rdd.getNumPartitions()\n",
    "#crime_rdd.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a11b7af-139a-4440-a048-e58f01d044ea",
   "metadata": {},
   "source": [
    "<h4>Total crimes in each borough (interested in the top10)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "114dbf92-93b8-4433-89ea-884ffc42acb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Westminster', 455028), ('Lambeth', 292178), ('Southwark', 278809), ('Camden', 275147), ('Newham', 262024), ('Croydon', 260294), ('Ealing', 251562), ('Islington', 230286), ('Tower Hamlets', 228613), ('Brent', 227551)]\n",
      "Elapsed time: 207.5611755847931 seconds\n"
     ]
    }
   ],
   "source": [
    "# Start the timer \n",
    "start_time = time.time()\n",
    "\n",
    "crimes_inborou = crime_rdd.map(lambda doc: (doc.borough,doc.value)).reduceByKey(lambda a, b: a + b).sortBy(lambda t4tuple:t4tuple[1],ascending=False)\n",
    "crimes_inborou.persist()\n",
    "print(crimes_inborou.take(10))\n",
    "#crimes_inborou.unpersist()\n",
    "\n",
    "# End the timer \n",
    "end_time = time.time() # Calculate elapsed time \n",
    "elapsed_time = end_time - start_time \n",
    "print(f\"Elapsed time: {end_time - start_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a47c2a4-92a6-44d2-8515-f20443d7f196",
   "metadata": {},
   "source": [
    "We see that some of the boroughs with bad reputation like Westminster and Camden are in the top 10 indeed while other areas like Hackney which had a bad reputation for years , is not in the top 10 boroughs in terms of crimes committed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c56cf8-064b-462f-996d-7ea42614ff4f",
   "metadata": {},
   "source": [
    "<h4>Top crimes in all boroughs throughout the years (interested in the top5)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5e339f0b-7222-4371-b8b3-5d014e6fa320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Theft and Handling', 2661861), ('Violence Against the Person', 1558081), ('Burglary', 754293), ('Criminal Damage', 630938), ('Drugs', 470765)]\n",
      "Elapsed time: 236.7077271938324 seconds\n"
     ]
    }
   ],
   "source": [
    "# Start the timer \n",
    "start_time = time.time()\n",
    "\n",
    "top_crimes_major = crime_rdd.map(lambda doc: (doc.major_category,doc.value)).reduceByKey(lambda a, b: a + b).sortBy(lambda t:t[1],ascending=False)\n",
    "top_crimes_major.persist()\n",
    "print(top_crimes_major.take(5))\n",
    "\n",
    "# End the timer \n",
    "end_time = time.time() # Calculate elapsed time \n",
    "#elapsed_time = end_time - start_time \n",
    "print(f\"Elapsed time: {end_time - start_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b2875d-14d5-424f-8d3c-7d1a76eb921b",
   "metadata": {},
   "source": [
    "We can see that 'Theft and Handling' and is the top crime in London. Since London is a tourist hotspot, it makes sense theft (pickpocketing) to be amongst the top crimes. With Westminster and Camden being amongst the most visited areas by tourists and amonst the top boroughs with the most crimes, we can maybe check if 'Theft and Handling' is amongst the top crimes in those specific boroughs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ef352c-7208-4f1f-b575-1d283d8affc3",
   "metadata": {},
   "source": [
    "<h4>Total crimes in each year (interested in the top3)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6c2bd74d-55ec-45d1-9cb1-6fa850de8d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2008, 738641), (2012, 737329), (2016, 736121)]\n",
      "Elapsed time: 239.12691974639893 seconds\n"
     ]
    }
   ],
   "source": [
    "# Start the timer \n",
    "start_time = time.time()\n",
    "\n",
    "crimes_peryear = crime_rdd.map(lambda doc: (doc.year,doc.value)).reduceByKey(lambda a, b: a + b).sortBy(lambda t:t[1],ascending=False)\n",
    "crimes_peryear.persist()\n",
    "print(crimes_peryear.take(3))\n",
    "\n",
    "# End the timer \n",
    "end_time = time.time() # Calculate elapsed time \n",
    "#elapsed_time = end_time - start_time \n",
    "print(f\"Elapsed time: {end_time - start_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408257bf-6874-4f41-83f0-51ba1baf4d88",
   "metadata": {},
   "source": [
    "As we can see from the top 3 years in crimes, there is no clear pattern as to if the crimes increase or decrease in time since the top 3 years have each 4 years difference between them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8a6b93-1393-40cd-937f-27eef05c8329",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ffccbe-962d-404a-b83a-d88a507cdc5a",
   "metadata": {},
   "source": [
    "<h3> Spark Dataframe </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90defcee-5a86-4373-bbcf-3cc250619bff",
   "metadata": {},
   "source": [
    "We are going to use Spark's Dataframes to run the same 'queries' and retrieve the data we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1755b0c6-85b0-43cb-9f7d-341a25a6d2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- lsoa_code: string (nullable = true)\n",
      " |-- borough: string (nullable = true)\n",
      " |-- major_category: string (nullable = true)\n",
      " |-- minor_category: string (nullable = true)\n",
      " |-- value: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "crime_df.printSchema() #we take a look at the Dataframe scheme which was enforced earlier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17cd6e2-b6a8-4df7-bc63-4fe90f532e5b",
   "metadata": {},
   "source": [
    "<h4>Total crimes in each borough</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "17e2c5a8-81e9-4ad0-a446-ab7ab29e9ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|count(DISTINCT borough)|\n",
      "+-----------------------+\n",
      "|                     33|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "crime_df.select(countDistinct('borough')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2a50aaf9-0f7e-4a2c-9233-a237bcbba392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------------+\n",
      "|      borough|total_crimes_per_borough|\n",
      "+-------------+------------------------+\n",
      "|  Westminster|                  455028|\n",
      "|      Lambeth|                  292178|\n",
      "|    Southwark|                  278809|\n",
      "|       Camden|                  275147|\n",
      "|       Newham|                  262024|\n",
      "|      Croydon|                  260294|\n",
      "|       Ealing|                  251562|\n",
      "|    Islington|                  230286|\n",
      "|Tower Hamlets|                  228613|\n",
      "|        Brent|                  227551|\n",
      "+-------------+------------------------+\n",
      "\n",
      "None\n",
      "Elapsed time: 20.160347938537598 seconds\n"
     ]
    }
   ],
   "source": [
    "# Start the timer \n",
    "start_time = time.time()\n",
    "\n",
    "crimes_per_borough=crime_df.select(['borough','value'])\\\n",
    "                .groupBy(['borough'])\\\n",
    "                .agg(sum(\"value\").alias(\"total_crimes_per_borough\")) \\\n",
    "                .sort(desc('total_crimes_per_borough')) \\\n",
    "                .limit(10)\n",
    "print(crimes_per_borough.show())\n",
    "\n",
    "# End the timer \n",
    "end_time = time.time() # Calculate elapsed time \n",
    "#elapsed_time = end_time - start_time \n",
    "print(f\"Elapsed time: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8dbedd7-5bfc-4c27-b15b-1cf7a7ffa84b",
   "metadata": {},
   "source": [
    "We see that the results we get from the Spark dataframe are exactly the same we got from using Sparks RDD map and reduce functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9285fc79-7fce-472a-81c2-5314d5d1cb98",
   "metadata": {},
   "source": [
    "<h4>Top crimes in all boroughs throughout the years</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "da4ba959-fc33-489a-8040-ea8705499227",
   "metadata": {},
   "outputs": [],
   "source": [
    "#crime_df.select(countDistinct('major_category')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a36f3947-fe7b-4ea4-98c5-41d202a246ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get distinct elements of the 'major_category' column \n",
    "#distinct_major = crime_df.select(\"major_category\").distinct()\n",
    "#distinct_major.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c42c513c-ec6b-4519-b7d2-08318c109ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------------+\n",
      "|      major_category|total_crimes_per_category|\n",
      "+--------------------+-------------------------+\n",
      "|  Theft and Handling|                  2661861|\n",
      "|Violence Against ...|                  1558081|\n",
      "|            Burglary|                   754293|\n",
      "|     Criminal Damage|                   630938|\n",
      "|               Drugs|                   470765|\n",
      "+--------------------+-------------------------+\n",
      "\n",
      "None\n",
      "Elapsed time: 23.886985540390015 seconds\n"
     ]
    }
   ],
   "source": [
    "# Start the timer \n",
    "start_time = time.time()\n",
    "\n",
    "crimes_per_major=crime_df.select(['major_category','value'])\\\n",
    "                .groupBy(['major_category'])\\\n",
    "                .agg(sum(\"value\").alias(\"total_crimes_per_category\")) \\\n",
    "                .sort(desc('total_crimes_per_category')) \\\n",
    "                .limit(5)\n",
    "print(crimes_per_major.show())\n",
    "\n",
    "# End the timer \n",
    "end_time = time.time() # Calculate elapsed time \n",
    "#elapsed_time = end_time - start_time \n",
    "print(f\"Elapsed time: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78e5f33-5ed5-4ba5-9015-3701dcb344bc",
   "metadata": {},
   "source": [
    "<h4>Total crimes in each year</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "15f67b6f-72f6-44fc-8f42-4cd779652974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------------+\n",
      "|year|total_crimes_per_year|\n",
      "+----+---------------------+\n",
      "|2008|               738641|\n",
      "|2012|               737329|\n",
      "|2016|               736121|\n",
      "+----+---------------------+\n",
      "\n",
      "None\n",
      "Elapsed time: 17.48635506629944 seconds\n"
     ]
    }
   ],
   "source": [
    "# Start the timer \n",
    "start_time = time.time()\n",
    "\n",
    "crimes_per_year=crime_df.select(['year','value'])\\\n",
    "                .groupBy(['year'])\\\n",
    "                .agg(sum(\"value\").alias(\"total_crimes_per_year\")) \\\n",
    "                .sort(desc('total_crimes_per_year')) \\\n",
    "                .limit(3)\n",
    "print(crimes_per_year.show())\n",
    "\n",
    "# End the timer \n",
    "end_time = time.time() # Calculate elapsed time \n",
    "#elapsed_time = end_time - start_time \n",
    "print(f\"Elapsed time: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d53e12e-bc5b-4030-ac0d-511df4e5e8b9",
   "metadata": {},
   "source": [
    "#### <u>**Time comparison:**</u>\n",
    "We can see that Dataframes always take less time, perform better when executing the queries and give us the data we are asking. Dataframes are built on top of RDDs but they leverage optimizations such as Catalyst, Tungsten, and efficient memory management, which make them faster for many workloads compared to RDDs. Also, Dataframes handle certain optimizations by their self while in RDDs it is up to the developer to implement some of the optimizations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3e5662-69c9-4c6b-8557-9c3ccffee4a9",
   "metadata": {},
   "source": [
    "#### <u>**Conclusion:**</u>\n",
    "Besides the much better performance by the Dataframes it also feels more natural and easier to handle the Dataframes in comparison to the RDDs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb881cf5-1c52-4c54-8dc2-5a93a72b4ed8",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8842d8-aa4a-4a98-a830-30cc9a116547",
   "metadata": {},
   "source": [
    "### Hypothesis Testing Examples\n",
    "For the last part of this notebook we will perform a couple of statistical tests as examples of the interesting information we can obtain from this dataset.\n",
    "The first one is a Chi-Square test of independence, which will test whether the major crime categories are dependent on the specific borough, using the differences in observed vs expected frequencies of the categorical variables.\n",
    "\n",
    "The underlying assumptions for the Chi-Square test of independence are actually met here, but, as we also mention on the next test we perform, the actual validity of the test is not the main focus here.\n",
    "\n",
    "Both tests where initially ran locally on this notebook, using a diminished version of the dataset, just to make sure that the code works and returnes valid results.\n",
    "\n",
    "However, since the main approach involves using these tests as separate .py executables, and running them on Dataproc job sessions, the final results from the whole dataset are stored on Dataproc as the respective outputs of the two jobs submitted.\n",
    "\n",
    "The preliminary results of the initial tests are not shown here, as they are completely irrelevant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f632fc02-871d-4578-bbc2-591e2b8f2dca",
   "metadata": {},
   "source": [
    "### Chi-Square Test of Independence (Major Crime Categories vs Boroughs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35796c4c-9adb-491f-8b9e-0b46003b241a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from google.api_core.retry import Retry\n",
    "\n",
    "#import findspark\n",
    "#findspark.init()\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.stat import Statistics\n",
    "from pyspark.mllib.linalg import Vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8a5b47-970b-49d5-bb06-1cd6de4befb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark Session for the Chi-Square Test of Independence\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Chi-Square Test\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load Dataset (The complete json file for this test)\n",
    "file_path = \"london_crime_json.json\"  # Dataset path\n",
    "crime_df = spark.read.json(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaecc514-a8b5-47d7-998e-8714300fa0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Contingency Table\n",
    "contingency_df = crime_df.groupBy(\"borough\", \"major_category\").count() \\\n",
    "                         .groupBy(\"borough\") \\\n",
    "                         .pivot(\"major_category\") \\\n",
    "                         .sum(\"count\") \\\n",
    "                         .fillna(0)\n",
    "\n",
    "# Convert all feature columns to DoubleType\n",
    "contingency_df = contingency_df.select(\n",
    "    [col(c).cast(DoubleType()).alias(c) if c != \"borough\" else col(c) for c in contingency_df.columns]\n",
    ")\n",
    "\n",
    "# Create a dictionary mapping boroughs to numeric values \n",
    "boroughs = contingency_df.select(\"borough\").distinct().rdd.flatMap(lambda x: x).collect() \n",
    "borough_mapping = {borough: idx for idx, borough in enumerate(boroughs)}\n",
    "\n",
    "# Convert Rows to LabeledPoint\n",
    "def row_to_labeled_point(row,borough_mapping):\n",
    "    label = borough_mapping[row[\"borough\"]] \n",
    "\n",
    "    features = [row[col] for col in row if col != \"borough\"]  # All other columns are features\n",
    "    \n",
    "    return (label,features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5645f1-683b-49be-97ca-1cd63044db75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking out the mapping for the boroughs\n",
    "borough_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c247d4b1-0f2e-4831-91cf-99618737cbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking out the contigency table\n",
    "contingency_df.limit(20).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc52c84-9965-4ee7-9c28-e95081792bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the contingency DataFrame to an RDD of LabeledPoint\n",
    "labeled_rdd = contingency_df.rdd.map(lambda row: row_to_labeled_point(row.asDict(),borough_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df144c6-06fd-418e-bd2d-26228dd1d263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data to RDD of LabeledPoint \n",
    "labeled_rdd2 = labeled_rdd.map(lambda x: LabeledPoint(x[0], Vectors.dense(x[1])))\n",
    "\n",
    "# Perform the Chi-Squared Test\n",
    "chi_sq_result = Statistics.chiSqTest(labeled_rdd2)\n",
    "\n",
    "# Print the results\n",
    "for i, result in enumerate(chi_sq_result):\n",
    "    print(f\"Feature {i + 1}:\")\n",
    "    print(f\"Chi-squared statistic: {result.statistic}\")\n",
    "    print(f\"p-value: {result.pValue}\")\n",
    "    print(f\"Degrees of freedom: {result.degreesOfFreedom}\")\n",
    "    print(f\"Method: {result.method}\\n\")\n",
    "\n",
    "# Stop SparkContext\n",
    "#sc.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f668f8ab-7843-4084-8138-9524d94be384",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efde402c-df0e-47d9-9999-ff9f14adcbde",
   "metadata": {},
   "source": [
    "### T-test between the top 2 Boroughs (for the total crime means)\n",
    "This test will compare the total crime means of the top 2 boroughs, so we can see whether their difference is statistically significant or not.\n",
    "Basically it is a Welch's T-test assuming heteroscedasticity (equal_var = False), because we don't actually know the true variances of the samples/population.\n",
    "\n",
    "Assumptions for the T-test (such as Normality) were also not tested here, although there are ways one should do this, prior to testing (Levene's test for homoscedasticity, Anderson-Darling test for Normality, etc). \n",
    "The point is that the focus here is not exactly the validity of these tests itself, but the general big data methodology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769a8175-5c99-4886-8d8c-2d287bb00a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum as _sum\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Initialize Spark Session for the T-Test\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TTestCrimeMeans\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load Dataset (The complete json file for this test)\n",
    "file_path = \"london_crime_json.json\"  # Dataset path\n",
    "crime_df = spark.read.json(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d13d33-67db-4916-8614-bd7ac8554c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Total Crimes per Borough\n",
    "# We need to do this in order to identify the top 2 boroughs in total crime incidences\n",
    "total_crimes = crime_df.groupBy(\"borough\").agg(_sum(\"value\").alias(\"total_crimes\"))\n",
    "\n",
    "# Identify the Top 2 Boroughs\n",
    "top_boroughs = total_crimes.orderBy(col(\"total_crimes\").desc()).limit(2).collect()\n",
    "top_borough_1 = top_boroughs[0][\"borough\"]\n",
    "top_borough_2 = top_boroughs[1][\"borough\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956a8cb4-705c-4322-952f-b786ef1a62b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter Crime Values for the Top Boroughs\n",
    "borough_1_crimes = crime_df.filter(col(\"borough\") == top_borough_1).select(\"value\").rdd.flatMap(lambda x: x).collect()\n",
    "borough_2_crimes = crime_df.filter(col(\"borough\") == top_borough_2).select(\"value\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Perform the T-Test\n",
    "t_stat, p_value = ttest_ind(borough_1_crimes, borough_2_crimes, equal_var=False)\n",
    "\n",
    "# Print Test Results\n",
    "print(\"T-Test Results:\")\n",
    "print(f\"T-Statistic: {t_stat}\")\n",
    "print(f\"P-Value: {p_value}\")\n",
    "print(f\"Top Borough 1: {top_borough_1}\")\n",
    "print(f\"Top Borough 2: {top_borough_2}\")\n",
    "\n",
    "# Interpret the Results\n",
    "if p_value < 0.05:\n",
    "    print(\"The difference in mean total crimes between the two boroughs is statistically significant.\")\n",
    "else:\n",
    "    print(\"The difference in mean total crimes between the two boroughs is not statistically significant.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1e3bb4-8ba7-4d59-9013-59bbc473136e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
